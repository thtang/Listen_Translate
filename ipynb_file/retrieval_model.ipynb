{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import seq2seq\n",
    "# from seq2seq.models import SimpleSeq2Seq\n",
    "import keras.backend as K\n",
    "import gensim\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, GRU, Embedding, Bidirectional, BatchNormalization, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import History ,ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.merge import add, dot, concatenate\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "%env CUDA_VISIBLE_DEVICES=7\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45036 個 training 音檔\n",
      "2000 個 testing 音檔\n",
      "max langth of wav: 246\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(\"data/train.data\")\n",
    "test_data = np.load(\"data/test.data\")\n",
    "print(len(train_data),\"個 training 音檔\")\n",
    "print(len(test_data),\"個 testing 音檔\")\n",
    "max_frame_length = np.max([len(sample) for sample in train_data])\n",
    "print(\"max langth of wav:\",max_frame_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load caption\n",
    "with open(\"data/train.caption\",\"r\") as f:\n",
    "    train_caption = f.readlines()\n",
    "    train_caption = [sent.strip() for sent in train_caption]\n",
    "    train_sentences = [sent.split(\" \") for sent in train_caption]\n",
    "with open(\"data/test.csv\",\"r\") as f:\n",
    "    test_choice = f.readlines()\n",
    "    test_choice = [sent.strip() for sent in test_choice]\n",
    "    test_corpus = \",\".join(test_choice)\n",
    "    test_sentences = [sent.split(\" \") for sent in test_corpus.split(\",\")]\n",
    "    test_corpus = test_corpus.replace(\",\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of token in caption: 2446\n"
     ]
    }
   ],
   "source": [
    "# chinese character level tokenizer\n",
    "tokenizer = Tokenizer(num_words=None,filters='\\n', lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(train_caption + [test_corpus])\n",
    "print(\"number of token in caption:\", len(tokenizer.word_index))\n",
    "inv_map = {v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 13\n"
     ]
    }
   ],
   "source": [
    "train_caption_sequences = tokenizer.texts_to_sequences(train_caption)\n",
    "max_length = np.max([len(i) for i in train_caption_sequences])\n",
    "print(\"max length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['對', '了', '文', '隆', '哥']\n",
      "(45036, 13)\n",
      "(45036, 246, 39)\n"
     ]
    }
   ],
   "source": [
    "# pad sequence\n",
    "train_caption_pad = pad_sequences(train_caption_sequences, maxlen=max_length)\n",
    "train_data_pad = pad_sequences(train_data, maxlen=max_frame_length,dtype='float32')\n",
    "# revert\n",
    "print([inv_map[i] for i in  train_caption_pad[1] if i != 0])\n",
    "print(train_caption_pad.shape)\n",
    "print(train_data_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # build embedding matrix\n",
    "# embedding_matrix = np.zeros((len(tokenizer.word_index)+1, emb_size))\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     try:\n",
    "#         embedding_vector = w2v_model.wv[word]\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "#     except:\n",
    "#         oov_count +=1\n",
    "#         print(word)\n",
    "# print(\"embedding matrix shape:\",embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build training tensor (truth and fake for binary calssification)\n",
    "sent_word_count = np.count_nonzero(train_caption_pad,axis=1)\n",
    "false_caption = []\n",
    "false_mfcc = train_data_pad\n",
    "true_caption = train_caption_pad\n",
    "true_mfcc = train_data_pad\n",
    "\n",
    "# sample fake caption with same length\n",
    "# for i,sent in enumerate(train_caption_pad):\n",
    "#     current_word_count = sent_word_count[i]\n",
    "#     fake_sent = train_caption_pad[np.random.choice(np.where(sent_word_count==current_word_count)[0],1)]\n",
    "#     if list(fake_sent) != list(sent):\n",
    "#         false_caption += list(fake_sent)\n",
    "#     else:\n",
    "#         fake_sent = train_caption_pad[np.random.choice(np.where(sent_word_count==current_word_count)[0],1)]\n",
    "#         false_caption += list(fake_sent)\n",
    "#         print(\"replicate sample\")\n",
    "\n",
    "## rolling way\n",
    "false_caption = np.concatenate((np.roll(train_caption_pad,1,axis=0),\n",
    "                                np.roll(train_caption_pad,2,axis=0),\n",
    "                               np.roll(train_caption_pad,3,axis=0),\n",
    "                               np.roll(train_caption_pad,4,axis=0),\n",
    "                               np.roll(train_caption_pad,5,axis=0)))\n",
    "false_mfcc = np.concatenate((train_data_pad,\n",
    "                             train_data_pad,\n",
    "                             train_data_pad,\n",
    "                             train_data_pad,\n",
    "                             train_data_pad))\n",
    "true_caption = train_caption_pad\n",
    "true_mfcc = train_data_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for _ in range(1):\n",
    "#     false_mfcc += list(train_data_pad)\n",
    "#     np.random.shuffle(_train_caption_pad)\n",
    "#     false_caption += list(_train_caption_pad)\n",
    "ground_truth = [ 1 for _ in range(len(true_caption))] + [0 for _ in range(len(false_caption))]\n",
    "train_mfcc = np.concatenate((true_mfcc, np.array(false_mfcc)))\n",
    "train_caption = np.concatenate((true_caption, np.array(false_caption)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270216, 246, 39)\n",
      "(270216, 13)\n"
     ]
    }
   ],
   "source": [
    "print(train_mfcc.shape)\n",
    "print(train_caption.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0  48  16   3   2 939 872   1]\n",
      "where [    1    17    39 ..., 45017 45018 45026]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3033])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_caption_pad[4])\n",
    "print(\"where\",np.where(np.count_nonzero(train_caption_pad,axis=1)==5)[0])\n",
    "np.random.choice(np.where(np.count_nonzero(train_caption_pad,axis=1)==5)[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 13, 100)       244700      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 246, 256)      172032      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional)  (None, 13, 256)       234496      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 246, 128)      164352      bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 13, 128)       164352      bidirectional_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional)  (None, 64)            41216       bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional)  (None, 64)            41216       bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1)             0           bidirectional_3[0][0]            \n",
      "                                                                   bidirectional_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             2           dot_1[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1076s - loss: 0.4540 - acc: 0.8317 - val_loss: 0.4360 - val_acc: 0.8351\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 1108s - loss: 0.4205 - acc: 0.8332 - val_loss: 0.4039 - val_acc: 0.8360\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 985s - loss: 0.3948 - acc: 0.8339 - val_loss: 0.3804 - val_acc: 0.8373\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 987s - loss: 0.3785 - acc: 0.8352 - val_loss: 0.3666 - val_acc: 0.8381\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 985s - loss: 0.3676 - acc: 0.8366 - val_loss: 0.3619 - val_acc: 0.8420\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 979s - loss: 0.3581 - acc: 0.8393 - val_loss: 0.3471 - val_acc: 0.8441\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 983s - loss: 0.3519 - acc: 0.8409 - val_loss: 0.3459 - val_acc: 0.8456\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 986s - loss: 0.3400 - acc: 0.8448 - val_loss: 0.3352 - val_acc: 0.8493\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1026s - loss: 0.3323 - acc: 0.8478 - val_loss: 0.3278 - val_acc: 0.8501\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 1086s - loss: 0.3265 - acc: 0.8505 - val_loss: 0.3266 - val_acc: 0.8522\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 1135s - loss: 0.3202 - acc: 0.8533 - val_loss: 0.3239 - val_acc: 0.8537\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 1053s - loss: 0.3145 - acc: 0.8565 - val_loss: 0.3179 - val_acc: 0.8586\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 1067s - loss: 0.3082 - acc: 0.8599 - val_loss: 0.3163 - val_acc: 0.8610\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 986s - loss: 0.3025 - acc: 0.8616 - val_loss: 0.3118 - val_acc: 0.8609\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 983s - loss: 0.2967 - acc: 0.8658 - val_loss: 0.3103 - val_acc: 0.8612\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 984s - loss: 0.2905 - acc: 0.8687 - val_loss: 0.3124 - val_acc: 0.8637\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 985s - loss: 0.2849 - acc: 0.8713 - val_loss: 0.3071 - val_acc: 0.8649\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 977s - loss: 0.2794 - acc: 0.8745 - val_loss: 0.3041 - val_acc: 0.8686\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 1022s - loss: 0.2734 - acc: 0.8784 - val_loss: 0.3023 - val_acc: 0.8677\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 1132s - loss: 0.2670 - acc: 0.8809 - val_loss: 0.3048 - val_acc: 0.8703\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 1098s - loss: 0.2603 - acc: 0.8846 - val_loss: 0.3065 - val_acc: 0.8684\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 1116s - loss: 0.2571 - acc: 0.8858 - val_loss: 0.3016 - val_acc: 0.8715\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 1100s - loss: 0.2501 - acc: 0.8902 - val_loss: 0.2993 - val_acc: 0.8723\n",
      "Epoch 24/50\n",
      "256705/256705 [==============================] - 1128s - loss: 0.2448 - acc: 0.8924 - val_loss: 0.2986 - val_acc: 0.8743\n",
      "Epoch 25/50\n",
      "256705/256705 [==============================] - 1096s - loss: 0.2401 - acc: 0.8951 - val_loss: 0.2946 - val_acc: 0.8763\n",
      "Epoch 26/50\n",
      "256705/256705 [==============================] - 1111s - loss: 0.2353 - acc: 0.8979 - val_loss: 0.3008 - val_acc: 0.8757\n",
      "Epoch 27/50\n",
      "256705/256705 [==============================] - 1090s - loss: 0.2297 - acc: 0.8996 - val_loss: 0.3004 - val_acc: 0.8757\n",
      "Epoch 28/50\n",
      "256705/256705 [==============================] - 1096s - loss: 0.2269 - acc: 0.9011 - val_loss: 0.3012 - val_acc: 0.8782\n",
      "Epoch 29/50\n",
      "256705/256705 [==============================] - 1097s - loss: 0.2215 - acc: 0.9039 - val_loss: 0.3022 - val_acc: 0.8774\n",
      "Epoch 30/50\n",
      "256705/256705 [==============================] - 1115s - loss: 0.2167 - acc: 0.9059 - val_loss: 0.3072 - val_acc: 0.8754\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 13, 100)       244700      input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional)  (None, 246, 256)      172032      input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional) (None, 13, 256)       234496      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional)  (None, 246, 128)      164352      bidirectional_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional) (None, 13, 128)       164352      bidirectional_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional)  (None, 64)            41216       bidirectional_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional) (None, 64)            41216       bidirectional_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_2 (Dot)                      (None, 1)             0           bidirectional_9[0][0]            \n",
      "                                                                   bidirectional_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             2           dot_2[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1119s - loss: 0.4429 - acc: 0.8322 - val_loss: 0.4134 - val_acc: 0.8318\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 1122s - loss: 0.4029 - acc: 0.8334 - val_loss: 0.3909 - val_acc: 0.8332\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 1098s - loss: 0.3903 - acc: 0.8339 - val_loss: 0.3913 - val_acc: 0.8327\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 1102s - loss: 0.3827 - acc: 0.8340 - val_loss: 0.3821 - val_acc: 0.8360\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 1089s - loss: 0.3740 - acc: 0.8351 - val_loss: 0.3712 - val_acc: 0.8356\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 1097s - loss: 0.3597 - acc: 0.8380 - val_loss: 0.3568 - val_acc: 0.8390\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 1096s - loss: 0.3462 - acc: 0.8416 - val_loss: 0.3438 - val_acc: 0.8428\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 1098s - loss: 0.3363 - acc: 0.8449 - val_loss: 0.3474 - val_acc: 0.8415\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1105s - loss: 0.3270 - acc: 0.8495 - val_loss: 0.3386 - val_acc: 0.8456\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 1101s - loss: 0.3189 - acc: 0.8540 - val_loss: 0.3356 - val_acc: 0.8457\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 1099s - loss: 0.3119 - acc: 0.8573 - val_loss: 0.3288 - val_acc: 0.8502\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 1095s - loss: 0.3048 - acc: 0.8608 - val_loss: 0.3311 - val_acc: 0.8515\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 1091s - loss: 0.2993 - acc: 0.8634 - val_loss: 0.3285 - val_acc: 0.8511\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 1028s - loss: 0.2952 - acc: 0.8665 - val_loss: 0.3216 - val_acc: 0.8557\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 1044s - loss: 0.2903 - acc: 0.8681 - val_loss: 0.3224 - val_acc: 0.8560\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 1128s - loss: 0.2850 - acc: 0.8718 - val_loss: 0.3171 - val_acc: 0.8589\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 1064s - loss: 0.2838 - acc: 0.8724 - val_loss: 0.3188 - val_acc: 0.8569\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 1141s - loss: 0.2786 - acc: 0.8751 - val_loss: 0.3188 - val_acc: 0.8589\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 1156s - loss: 0.2718 - acc: 0.8783 - val_loss: 0.3082 - val_acc: 0.8637\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 1110s - loss: 0.2685 - acc: 0.8807 - val_loss: 0.3113 - val_acc: 0.8640\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 1040s - loss: 0.2622 - acc: 0.8836 - val_loss: 0.3132 - val_acc: 0.8614\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 1028s - loss: 0.2571 - acc: 0.8859 - val_loss: 0.3106 - val_acc: 0.8642\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 1080s - loss: 0.2507 - acc: 0.8891 - val_loss: 0.3120 - val_acc: 0.8649\n",
      "Epoch 24/50\n",
      "256705/256705 [==============================] - 1122s - loss: 0.2464 - acc: 0.8914 - val_loss: 0.3140 - val_acc: 0.8664\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 13, 100)       244700      input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional) (None, 246, 256)      172032      input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional) (None, 13, 256)       234496      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional) (None, 246, 128)      164352      bidirectional_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional) (None, 13, 128)       164352      bidirectional_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional) (None, 64)            41216       bidirectional_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional) (None, 64)            41216       bidirectional_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_3 (Dot)                      (None, 1)             0           bidirectional_15[0][0]           \n",
      "                                                                   bidirectional_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             2           dot_3[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1195s - loss: 0.4458 - acc: 0.8319 - val_loss: 0.4340 - val_acc: 0.8293\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 1128s - loss: 0.4140 - acc: 0.8335 - val_loss: 0.4025 - val_acc: 0.8288\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 1110s - loss: 0.3933 - acc: 0.8340 - val_loss: 0.3947 - val_acc: 0.8308\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 1096s - loss: 0.3788 - acc: 0.8354 - val_loss: 0.3757 - val_acc: 0.8332\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 1213s - loss: 0.3659 - acc: 0.8377 - val_loss: 0.3732 - val_acc: 0.8334\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 971s - loss: 0.3569 - acc: 0.8393 - val_loss: 0.3603 - val_acc: 0.8350\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 993s - loss: 0.3470 - acc: 0.8419 - val_loss: 0.3494 - val_acc: 0.8398\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 1017s - loss: 0.3355 - acc: 0.8462 - val_loss: 0.3391 - val_acc: 0.8448\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1005s - loss: 0.3269 - acc: 0.8505 - val_loss: 0.3342 - val_acc: 0.8457\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 1001s - loss: 0.3191 - acc: 0.8535 - val_loss: 0.3277 - val_acc: 0.8503\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 1004s - loss: 0.3098 - acc: 0.8584 - val_loss: 0.3267 - val_acc: 0.8491\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 1023s - loss: 0.3029 - acc: 0.8620 - val_loss: 0.3224 - val_acc: 0.8527\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 1096s - loss: 0.2946 - acc: 0.8667 - val_loss: 0.3235 - val_acc: 0.8540\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 1105s - loss: 0.2912 - acc: 0.8690 - val_loss: 0.3086 - val_acc: 0.8612\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 1133s - loss: 0.2834 - acc: 0.8724 - val_loss: 0.3101 - val_acc: 0.8623\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256705/256705 [==============================] - 1136s - loss: 0.2752 - acc: 0.8774 - val_loss: 0.3069 - val_acc: 0.8625\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 1121s - loss: 0.2672 - acc: 0.8808 - val_loss: 0.2998 - val_acc: 0.8673\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 1131s - loss: 0.2642 - acc: 0.8828 - val_loss: 0.3065 - val_acc: 0.8663\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 1109s - loss: 0.2568 - acc: 0.8870 - val_loss: 0.3050 - val_acc: 0.8668\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 1077s - loss: 0.2503 - acc: 0.8901 - val_loss: 0.2970 - val_acc: 0.8696\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 1091s - loss: 0.2438 - acc: 0.8929 - val_loss: 0.2990 - val_acc: 0.8731\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 1087s - loss: 0.2375 - acc: 0.8969 - val_loss: 0.2985 - val_acc: 0.8743\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 1075s - loss: 0.2323 - acc: 0.8983 - val_loss: 0.2989 - val_acc: 0.8748\n",
      "Epoch 24/50\n",
      "256705/256705 [==============================] - 1080s - loss: 0.2280 - acc: 0.9012 - val_loss: 0.3041 - val_acc: 0.8761\n",
      "Epoch 25/50\n",
      "256705/256705 [==============================] - 1074s - loss: 0.2217 - acc: 0.9035 - val_loss: 0.3010 - val_acc: 0.8774\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_7 (InputLayer)             (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 13, 100)       244700      input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional) (None, 246, 256)      172032      input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional) (None, 13, 256)       234496      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional) (None, 246, 128)      164352      bidirectional_19[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional) (None, 13, 128)       164352      bidirectional_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional) (None, 64)            41216       bidirectional_20[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional) (None, 64)            41216       bidirectional_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_4 (Dot)                      (None, 1)             0           bidirectional_21[0][0]           \n",
      "                                                                   bidirectional_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             2           dot_4[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1062s - loss: 0.4469 - acc: 0.8318 - val_loss: 0.4231 - val_acc: 0.8344\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 1058s - loss: 0.4061 - acc: 0.8332 - val_loss: 0.3918 - val_acc: 0.8344\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 1057s - loss: 0.3887 - acc: 0.8336 - val_loss: 0.4017 - val_acc: 0.8320\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 1055s - loss: 0.3807 - acc: 0.8347 - val_loss: 0.3795 - val_acc: 0.8363\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 1046s - loss: 0.3704 - acc: 0.8357 - val_loss: 0.3684 - val_acc: 0.8387\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 1044s - loss: 0.3576 - acc: 0.8380 - val_loss: 0.3612 - val_acc: 0.8414\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 1053s - loss: 0.3553 - acc: 0.8380 - val_loss: 0.3568 - val_acc: 0.8394\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 1055s - loss: 0.3427 - acc: 0.8420 - val_loss: 0.3460 - val_acc: 0.8409\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1043s - loss: 0.3324 - acc: 0.8465 - val_loss: 0.3464 - val_acc: 0.8435\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 1047s - loss: 0.3243 - acc: 0.8504 - val_loss: 0.3432 - val_acc: 0.8468\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 1051s - loss: 0.3161 - acc: 0.8540 - val_loss: 0.3335 - val_acc: 0.8481\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 962s - loss: 0.3097 - acc: 0.8578 - val_loss: 0.3291 - val_acc: 0.8529\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 939s - loss: 0.3036 - acc: 0.8614 - val_loss: 0.3251 - val_acc: 0.8534\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 938s - loss: 0.2963 - acc: 0.8647 - val_loss: 0.3253 - val_acc: 0.8550\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 927s - loss: 0.2904 - acc: 0.8682 - val_loss: 0.3247 - val_acc: 0.8548\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 933s - loss: 0.2846 - acc: 0.8711 - val_loss: 0.3209 - val_acc: 0.8597\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 926s - loss: 0.2791 - acc: 0.8746 - val_loss: 0.3159 - val_acc: 0.8619\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 935s - loss: 0.2719 - acc: 0.8788 - val_loss: 0.3149 - val_acc: 0.8655\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 931s - loss: 0.2669 - acc: 0.8804 - val_loss: 0.3141 - val_acc: 0.8657\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 935s - loss: 0.2598 - acc: 0.8847 - val_loss: 0.3119 - val_acc: 0.8643\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 929s - loss: 0.2544 - acc: 0.8872 - val_loss: 0.3070 - val_acc: 0.8700\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 933s - loss: 0.2484 - acc: 0.8906 - val_loss: 0.3072 - val_acc: 0.8673\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 940s - loss: 0.2436 - acc: 0.8930 - val_loss: 0.3077 - val_acc: 0.8694\n",
      "Epoch 24/50\n",
      "256705/256705 [==============================] - 986s - loss: 0.2376 - acc: 0.8958 - val_loss: 0.3077 - val_acc: 0.8717\n",
      "Epoch 25/50\n",
      "256705/256705 [==============================] - 985s - loss: 0.2323 - acc: 0.8989 - val_loss: 0.3094 - val_acc: 0.8711\n",
      "Epoch 26/50\n",
      "256705/256705 [==============================] - 1000s - loss: 0.2265 - acc: 0.9014 - val_loss: 0.3129 - val_acc: 0.8702\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_10 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_9 (InputLayer)             (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 13, 100)       244700      input_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_25 (Bidirectional) (None, 246, 256)      172032      input_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_28 (Bidirectional) (None, 13, 256)       234496      embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_26 (Bidirectional) (None, 246, 128)      164352      bidirectional_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_29 (Bidirectional) (None, 13, 128)       164352      bidirectional_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_27 (Bidirectional) (None, 64)            41216       bidirectional_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_30 (Bidirectional) (None, 64)            41216       bidirectional_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_5 (Dot)                      (None, 1)             0           bidirectional_27[0][0]           \n",
      "                                                                   bidirectional_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             2           dot_5[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 996s - loss: 0.4410 - acc: 0.8319 - val_loss: 0.4167 - val_acc: 0.8332\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 994s - loss: 0.4070 - acc: 0.8333 - val_loss: 0.3950 - val_acc: 0.8328\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 1021s - loss: 0.3882 - acc: 0.8341 - val_loss: 0.3867 - val_acc: 0.8312\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 986s - loss: 0.3742 - acc: 0.8354 - val_loss: 0.3704 - val_acc: 0.8361\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 980s - loss: 0.3609 - acc: 0.8378 - val_loss: 0.3585 - val_acc: 0.8402\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 1018s - loss: 0.3456 - acc: 0.8429 - val_loss: 0.3484 - val_acc: 0.8417\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 1025s - loss: 0.3346 - acc: 0.8475 - val_loss: 0.3470 - val_acc: 0.8420\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 1006s - loss: 0.3286 - acc: 0.8499 - val_loss: 0.3414 - val_acc: 0.8489\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1004s - loss: 0.3186 - acc: 0.8548 - val_loss: 0.3342 - val_acc: 0.8520\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 995s - loss: 0.3103 - acc: 0.8589 - val_loss: 0.3298 - val_acc: 0.8511\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 991s - loss: 0.3038 - acc: 0.8624 - val_loss: 0.3292 - val_acc: 0.8543\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 1046s - loss: 0.2960 - acc: 0.8669 - val_loss: 0.3242 - val_acc: 0.8578\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 1054s - loss: 0.2885 - acc: 0.8703 - val_loss: 0.3233 - val_acc: 0.8606\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 1033s - loss: 0.2821 - acc: 0.8742 - val_loss: 0.3143 - val_acc: 0.8632\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 987s - loss: 0.2749 - acc: 0.8777 - val_loss: 0.3170 - val_acc: 0.8629\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 1002s - loss: 0.2685 - acc: 0.8810 - val_loss: 0.3153 - val_acc: 0.8653\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 986s - loss: 0.2616 - acc: 0.8847 - val_loss: 0.3124 - val_acc: 0.8668\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 989s - loss: 0.2555 - acc: 0.8870 - val_loss: 0.3165 - val_acc: 0.8662\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 992s - loss: 0.2510 - acc: 0.8898 - val_loss: 0.3137 - val_acc: 0.8668\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 986s - loss: 0.2438 - acc: 0.8923 - val_loss: 0.3061 - val_acc: 0.8725\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 984s - loss: 0.2389 - acc: 0.8952 - val_loss: 0.3091 - val_acc: 0.8729\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 960s - loss: 0.2321 - acc: 0.8987 - val_loss: 0.3114 - val_acc: 0.8741\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 984s - loss: 0.2278 - acc: 0.9007 - val_loss: 0.3091 - val_acc: 0.8755\n",
      "Epoch 24/50\n",
      "256705/256705 [==============================] - 988s - loss: 0.2228 - acc: 0.9031 - val_loss: 0.3114 - val_acc: 0.8760\n",
      "Epoch 25/50\n",
      "256705/256705 [==============================] - 1002s - loss: 0.2184 - acc: 0.9049 - val_loss: 0.3148 - val_acc: 0.8754\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_12 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_11 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)          (None, 13, 100)       244700      input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_31 (Bidirectional) (None, 246, 256)      172032      input_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_34 (Bidirectional) (None, 13, 256)       234496      embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_32 (Bidirectional) (None, 246, 128)      164352      bidirectional_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_35 (Bidirectional) (None, 13, 128)       164352      bidirectional_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_33 (Bidirectional) (None, 64)            41216       bidirectional_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_36 (Bidirectional) (None, 64)            41216       bidirectional_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_6 (Dot)                      (None, 1)             0           bidirectional_33[0][0]           \n",
      "                                                                   bidirectional_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 1)             2           dot_6[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1064s - loss: 0.4456 - acc: 0.8316 - val_loss: 0.4233 - val_acc: 0.8322\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 959s - loss: 0.4097 - acc: 0.8336 - val_loss: 0.3922 - val_acc: 0.8306\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 947s - loss: 0.3874 - acc: 0.8349 - val_loss: 0.3770 - val_acc: 0.8353\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 941s - loss: 0.3731 - acc: 0.8360 - val_loss: 0.3692 - val_acc: 0.8325\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 932s - loss: 0.3624 - acc: 0.8379 - val_loss: 0.3625 - val_acc: 0.8374\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 1002s - loss: 0.3518 - acc: 0.8408 - val_loss: 0.3554 - val_acc: 0.8375\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 1007s - loss: 0.3416 - acc: 0.8438 - val_loss: 0.3479 - val_acc: 0.8430\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 1044s - loss: 0.3314 - acc: 0.8485 - val_loss: 0.3442 - val_acc: 0.8409\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1036s - loss: 0.3231 - acc: 0.8524 - val_loss: 0.3348 - val_acc: 0.8485\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 1055s - loss: 0.3139 - acc: 0.8573 - val_loss: 0.3284 - val_acc: 0.8540\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 988s - loss: 0.3063 - acc: 0.8620 - val_loss: 0.3195 - val_acc: 0.8586\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 985s - loss: 0.3050 - acc: 0.8632 - val_loss: 0.3184 - val_acc: 0.8571\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 1078s - loss: 0.2978 - acc: 0.8669 - val_loss: 0.3123 - val_acc: 0.8629\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 1051s - loss: 0.2885 - acc: 0.8713 - val_loss: 0.3077 - val_acc: 0.8674\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 1046s - loss: 0.2805 - acc: 0.8760 - val_loss: 0.3079 - val_acc: 0.8646\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256705/256705 [==============================] - 1048s - loss: 0.2750 - acc: 0.8782 - val_loss: 0.3057 - val_acc: 0.8696\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 1167s - loss: 0.2663 - acc: 0.8822 - val_loss: 0.3018 - val_acc: 0.8704\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 1021s - loss: 0.2605 - acc: 0.8856 - val_loss: 0.3054 - val_acc: 0.8674\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 948s - loss: 0.2548 - acc: 0.8885 - val_loss: 0.3127 - val_acc: 0.8679\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 946s - loss: 0.2487 - acc: 0.8911 - val_loss: 0.3046 - val_acc: 0.8700\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 950s - loss: 0.2428 - acc: 0.8944 - val_loss: 0.3034 - val_acc: 0.8717\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 922s - loss: 0.2375 - acc: 0.8967 - val_loss: 0.3056 - val_acc: 0.8715\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_14 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_13 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)          (None, 13, 100)       244700      input_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_37 (Bidirectional) (None, 246, 256)      172032      input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_40 (Bidirectional) (None, 13, 256)       234496      embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_38 (Bidirectional) (None, 246, 128)      164352      bidirectional_37[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_41 (Bidirectional) (None, 13, 128)       164352      bidirectional_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_39 (Bidirectional) (None, 64)            41216       bidirectional_38[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_42 (Bidirectional) (None, 64)            41216       bidirectional_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_7 (Dot)                      (None, 1)             0           bidirectional_39[0][0]           \n",
      "                                                                   bidirectional_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 1)             2           dot_7[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1046s - loss: 0.4474 - acc: 0.8315 - val_loss: 0.4236 - val_acc: 0.8352\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 1053s - loss: 0.4125 - acc: 0.8331 - val_loss: 0.3920 - val_acc: 0.8359\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 1073s - loss: 0.3940 - acc: 0.8340 - val_loss: 0.3836 - val_acc: 0.8354\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 1028s - loss: 0.3811 - acc: 0.8350 - val_loss: 0.3727 - val_acc: 0.8373\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 1041s - loss: 0.3698 - acc: 0.8361 - val_loss: 0.3615 - val_acc: 0.8362\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 1030s - loss: 0.3606 - acc: 0.8380 - val_loss: 0.3556 - val_acc: 0.8401\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 1057s - loss: 0.3510 - acc: 0.8407 - val_loss: 0.3506 - val_acc: 0.8417\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 1002s - loss: 0.3415 - acc: 0.8434 - val_loss: 0.3409 - val_acc: 0.8461\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1031s - loss: 0.3325 - acc: 0.8469 - val_loss: 0.3390 - val_acc: 0.8502\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 1009s - loss: 0.3247 - acc: 0.8508 - val_loss: 0.3354 - val_acc: 0.8488\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 992s - loss: 0.3174 - acc: 0.8552 - val_loss: 0.3322 - val_acc: 0.8534\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 1021s - loss: 0.3102 - acc: 0.8584 - val_loss: 0.3238 - val_acc: 0.8578\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 1020s - loss: 0.3032 - acc: 0.8626 - val_loss: 0.3169 - val_acc: 0.8606\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 1045s - loss: 0.2971 - acc: 0.8647 - val_loss: 0.3188 - val_acc: 0.8586\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 1029s - loss: 0.2901 - acc: 0.8691 - val_loss: 0.3166 - val_acc: 0.8591\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 1021s - loss: 0.2838 - acc: 0.8723 - val_loss: 0.3128 - val_acc: 0.8618\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 1028s - loss: 0.2777 - acc: 0.8749 - val_loss: 0.3153 - val_acc: 0.8627\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 1020s - loss: 0.2714 - acc: 0.8792 - val_loss: 0.3105 - val_acc: 0.8688\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 952s - loss: 0.2658 - acc: 0.8814 - val_loss: 0.3116 - val_acc: 0.8659\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 923s - loss: 0.2599 - acc: 0.8853 - val_loss: 0.3064 - val_acc: 0.8706\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 927s - loss: 0.2546 - acc: 0.8873 - val_loss: 0.3105 - val_acc: 0.8666\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 920s - loss: 0.2500 - acc: 0.8899 - val_loss: 0.3044 - val_acc: 0.8708\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 940s - loss: 0.2497 - acc: 0.8898 - val_loss: 0.3075 - val_acc: 0.8685\n",
      "Epoch 24/50\n",
      "256705/256705 [==============================] - 956s - loss: 0.2441 - acc: 0.8933 - val_loss: 0.3084 - val_acc: 0.8702\n",
      "Epoch 25/50\n",
      "256705/256705 [==============================] - 945s - loss: 0.2369 - acc: 0.8964 - val_loss: 0.3115 - val_acc: 0.8727\n",
      "Epoch 26/50\n",
      "256705/256705 [==============================] - 1092s - loss: 0.2327 - acc: 0.8988 - val_loss: 0.3058 - val_acc: 0.8751\n",
      "Epoch 27/50\n",
      "256705/256705 [==============================] - 1071s - loss: 0.2283 - acc: 0.9005 - val_loss: 0.3105 - val_acc: 0.8722\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_16 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_15 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 13, 100)       244700      input_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_43 (Bidirectional) (None, 246, 256)      172032      input_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_46 (Bidirectional) (None, 13, 256)       234496      embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_44 (Bidirectional) (None, 246, 128)      164352      bidirectional_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_47 (Bidirectional) (None, 13, 128)       164352      bidirectional_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_45 (Bidirectional) (None, 64)            41216       bidirectional_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_48 (Bidirectional) (None, 64)            41216       bidirectional_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_8 (Dot)                      (None, 1)             0           bidirectional_45[0][0]           \n",
      "                                                                   bidirectional_48[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             2           dot_8[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1055s - loss: 0.4421 - acc: 0.8317 - val_loss: 0.4272 - val_acc: 0.8335\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 951s - loss: 0.4200 - acc: 0.8335 - val_loss: 0.4136 - val_acc: 0.8338\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 961s - loss: 0.3991 - acc: 0.8339 - val_loss: 0.3911 - val_acc: 0.8329\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 963s - loss: 0.3809 - acc: 0.8349 - val_loss: 0.3760 - val_acc: 0.8372\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 961s - loss: 0.3648 - acc: 0.8369 - val_loss: 0.3613 - val_acc: 0.8361\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 966s - loss: 0.3491 - acc: 0.8415 - val_loss: 0.3491 - val_acc: 0.8408\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 962s - loss: 0.3350 - acc: 0.8470 - val_loss: 0.3378 - val_acc: 0.8445\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 1023s - loss: 0.3248 - acc: 0.8517 - val_loss: 0.3330 - val_acc: 0.8503\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1036s - loss: 0.3163 - acc: 0.8567 - val_loss: 0.3301 - val_acc: 0.8513\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 905s - loss: 0.3071 - acc: 0.8613 - val_loss: 0.3218 - val_acc: 0.8572\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 914s - loss: 0.2979 - acc: 0.8668 - val_loss: 0.3148 - val_acc: 0.8587\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 901s - loss: 0.2900 - acc: 0.8703 - val_loss: 0.3101 - val_acc: 0.8635\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 926s - loss: 0.2828 - acc: 0.8742 - val_loss: 0.3051 - val_acc: 0.8663\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 938s - loss: 0.2743 - acc: 0.8779 - val_loss: 0.3080 - val_acc: 0.8651\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 957s - loss: 0.2659 - acc: 0.8823 - val_loss: 0.3067 - val_acc: 0.8664\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 962s - loss: 0.2594 - acc: 0.8858 - val_loss: 0.3048 - val_acc: 0.8711\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 1034s - loss: 0.2533 - acc: 0.8889 - val_loss: 0.3068 - val_acc: 0.8684\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 976s - loss: 0.2464 - acc: 0.8925 - val_loss: 0.3053 - val_acc: 0.8688\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 1061s - loss: 0.2407 - acc: 0.8943 - val_loss: 0.3046 - val_acc: 0.8707\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 1101s - loss: 0.2360 - acc: 0.8974 - val_loss: 0.3058 - val_acc: 0.8737\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 1066s - loss: 0.2311 - acc: 0.8994 - val_loss: 0.3015 - val_acc: 0.8711\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 1051s - loss: 0.2257 - acc: 0.9019 - val_loss: 0.3106 - val_acc: 0.8697\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 1039s - loss: 0.2210 - acc: 0.9037 - val_loss: 0.3082 - val_acc: 0.8736\n",
      "Epoch 24/50\n",
      "256705/256705 [==============================] - 1043s - loss: 0.2155 - acc: 0.9067 - val_loss: 0.3094 - val_acc: 0.8720\n",
      "Epoch 25/50\n",
      "256705/256705 [==============================] - 1045s - loss: 0.2111 - acc: 0.9094 - val_loss: 0.3101 - val_acc: 0.8742\n",
      "Epoch 26/50\n",
      "256705/256705 [==============================] - 1044s - loss: 0.2072 - acc: 0.9106 - val_loss: 0.3157 - val_acc: 0.8740\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_18 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_17 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 13, 100)       244700      input_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_49 (Bidirectional) (None, 246, 256)      172032      input_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_52 (Bidirectional) (None, 13, 256)       234496      embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_50 (Bidirectional) (None, 246, 128)      164352      bidirectional_49[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_53 (Bidirectional) (None, 13, 128)       164352      bidirectional_52[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_51 (Bidirectional) (None, 64)            41216       bidirectional_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_54 (Bidirectional) (None, 64)            41216       bidirectional_53[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_9 (Dot)                      (None, 1)             0           bidirectional_51[0][0]           \n",
      "                                                                   bidirectional_54[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             2           dot_9[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,062,366\n",
      "Trainable params: 1,062,366\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1082s - loss: 0.4385 - acc: 0.8324 - val_loss: 0.4170 - val_acc: 0.8266\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 1053s - loss: 0.4012 - acc: 0.8337 - val_loss: 0.3985 - val_acc: 0.8278\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 1071s - loss: 0.3888 - acc: 0.8342 - val_loss: 0.3958 - val_acc: 0.8275\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 1066s - loss: 0.3820 - acc: 0.8346 - val_loss: 0.3879 - val_acc: 0.8290\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 1076s - loss: 0.3779 - acc: 0.8357 - val_loss: 0.3921 - val_acc: 0.8279\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 1080s - loss: 0.3726 - acc: 0.8358 - val_loss: 0.3778 - val_acc: 0.8300\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 1061s - loss: 0.3640 - acc: 0.8373 - val_loss: 0.3747 - val_acc: 0.8316\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 1055s - loss: 0.3548 - acc: 0.8389 - val_loss: 0.3720 - val_acc: 0.8314\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 1051s - loss: 0.3493 - acc: 0.8404 - val_loss: 0.3802 - val_acc: 0.8292\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 1037s - loss: 0.3471 - acc: 0.8413 - val_loss: 0.3523 - val_acc: 0.8358\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 1034s - loss: 0.3360 - acc: 0.8460 - val_loss: 0.3465 - val_acc: 0.8423\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 1033s - loss: 0.3265 - acc: 0.8497 - val_loss: 0.3426 - val_acc: 0.8417\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 995s - loss: 0.3186 - acc: 0.8537 - val_loss: 0.3319 - val_acc: 0.8458\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256705/256705 [==============================] - 1012s - loss: 0.3126 - acc: 0.8567 - val_loss: 0.3293 - val_acc: 0.8488\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 1027s - loss: 0.3062 - acc: 0.8593 - val_loss: 0.3316 - val_acc: 0.8482\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 1017s - loss: 0.2993 - acc: 0.8634 - val_loss: 0.3275 - val_acc: 0.8515\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 1063s - loss: 0.2943 - acc: 0.8664 - val_loss: 0.3266 - val_acc: 0.8549\n",
      "Epoch 18/50\n",
      " 19968/256705 [=>............................] - ETA: 985s - loss: 0.2869 - acc: 0.8703"
     ]
    }
   ],
   "source": [
    "# model\n",
    "emb_size = 100\n",
    "batch_size = 512\n",
    "epochs = 50\n",
    "for i in range(0,40):\n",
    "    mfcc_input = Input(shape=(train_mfcc.shape[1],train_mfcc.shape[2]))\n",
    "    mfcc_lstm1 = Bidirectional(LSTM(128,dropout=0.2, return_sequences=True))(mfcc_input)\n",
    "    mfcc_lstm2 = Bidirectional(LSTM(64,dropout=0.2, return_sequences=True))(mfcc_lstm1)\n",
    "    mfcc_lstm3 = Bidirectional(LSTM(32,dropout=0.2))(mfcc_lstm2)\n",
    "    \n",
    "    caption_input = Input(shape=(13,))\n",
    "    emb = Embedding(len(tokenizer.word_index)+1 ,output_dim= emb_size, \n",
    "                    input_length=max_length,trainable=True)(caption_input)\n",
    "    caption_lstm1 = Bidirectional(LSTM(128,dropout=0.2, return_sequences = True))(emb)\n",
    "    caption_lstm2 = Bidirectional(LSTM(64,dropout=0.2, return_sequences = True))(caption_lstm1)\n",
    "    caption_lstm3 = Bidirectional(LSTM(32,dropout=0.2))(caption_lstm2)\n",
    "    \n",
    "    merge = keras.layers.dot([mfcc_lstm3, caption_lstm3],1)\n",
    "    output_dense = Dense(1,activation=\"sigmoid\")(merge)\n",
    "    model = Model(inputs=[mfcc_input, caption_input], outputs=output_dense)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    # training\n",
    "    total_sample_size = len(ground_truth)\n",
    "    random_index = np.random.choice(total_sample_size,total_sample_size, replace=False)\n",
    "    \n",
    "    input_mfcc = train_mfcc[random_index]\n",
    "    input_caption = train_caption[random_index]\n",
    "    input_ground_truth = np.array(ground_truth)[random_index]\n",
    "\n",
    "    hist = History()\n",
    "#     check_save  = ModelCheckpoint(\"models/model_1v3-{epoch:05d}-{val_acc:.5f}.h5\",monitor='val_acc',save_best_only=True)\n",
    "    check_save  = ModelCheckpoint(\"models/model_1V5_3layers_\"+str(batch_size)+\"_\"+str(i)+\".h5\",monitor='val_acc',save_best_only=True)\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=4)\n",
    "    model.fit([input_mfcc, input_caption], input_ground_truth,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.05,\n",
    "             callbacks=[check_save, hist, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 13)\n",
      "(8000, 246, 39)\n"
     ]
    }
   ],
   "source": [
    "test_caption_sequences =  tokenizer.texts_to_sequences([\" \".join(sample) for sample in test_sentences])\n",
    "\n",
    "# pad sequence\n",
    "test_caption_pad = pad_sequences(test_caption_sequences, maxlen=max_length)\n",
    "test_data_pad = pad_sequences(test_data, maxlen=max_frame_length,dtype='float32')\n",
    "test_data_pad_expand = np.repeat(test_data_pad, 4,axis=0)\n",
    "# revert\n",
    "print(test_caption_pad.shape)\n",
    "print(test_data_pad_expand .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "p = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    model = load_model(\"models/model_1V5_512_\"+str(i)+\"_share1_bn.h5\")\n",
    "    prediction = model.predict([test_data_pad_expand,test_caption_pad])\n",
    "    p.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56048602],\n",
       "       [ 0.24864547],\n",
       "       [ 0.40250978],\n",
       "       ..., \n",
       "       [ 0.18335563],\n",
       "       [ 0.0273765 ],\n",
       "       [ 0.59960878]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction+prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load submit\n",
    "sample_submit = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "pred_y = np.argmax(pred_y_prob.reshape(-1,4),axis=1)\n",
    "sample_submit[\"answer\"] = pred_y\n",
    "sample_submit.to_csv(\"final_submission.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 13, 100)       244700      input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional) (None, 246, 256)      172032      input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional) (None, 13, 256)       234496      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional) (None, 128)           164352      bidirectional_13[0][0]           \n",
      "                                                                   bidirectional_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 128)           512         bidirectional_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 128)           512         bidirectional_12[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1)             0           batch_normalization_4[0][0]      \n",
      "                                                                   batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             2           dot_1[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1087s - loss: 0.5427 - acc: 0.7621 - val_loss: 0.5451 - val_acc: 0.7963\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 751s - loss: 0.4327 - acc: 0.8314 - val_loss: 2.4728 - val_acc: 0.5594\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 775s - loss: 0.4194 - acc: 0.8327 - val_loss: 1.5778 - val_acc: 0.5858\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 750s - loss: 0.4073 - acc: 0.8334 - val_loss: 0.4884 - val_acc: 0.7895\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 813s - loss: 0.3951 - acc: 0.8353 - val_loss: 0.5470 - val_acc: 0.7795\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 825s - loss: 0.3797 - acc: 0.8379 - val_loss: 0.4406 - val_acc: 0.8190\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 761s - loss: 0.3603 - acc: 0.8427 - val_loss: 0.4242 - val_acc: 0.8227\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 743s - loss: 0.3434 - acc: 0.8478 - val_loss: 0.4659 - val_acc: 0.8236\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 753s - loss: 0.3273 - acc: 0.8542 - val_loss: 0.3772 - val_acc: 0.8344\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 759s - loss: 0.3127 - acc: 0.8606 - val_loss: 0.3691 - val_acc: 0.8407\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 722s - loss: 0.3002 - acc: 0.8662 - val_loss: 0.3886 - val_acc: 0.8405\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 740s - loss: 0.2892 - acc: 0.8712 - val_loss: 0.3474 - val_acc: 0.8469\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 751s - loss: 0.2787 - acc: 0.8763 - val_loss: 0.3442 - val_acc: 0.8496\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 742s - loss: 0.2698 - acc: 0.8813 - val_loss: 0.3341 - val_acc: 0.8548\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 758s - loss: 0.2619 - acc: 0.8851 - val_loss: 0.3324 - val_acc: 0.8561\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 739s - loss: 0.2543 - acc: 0.8882 - val_loss: 0.3373 - val_acc: 0.8578\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 737s - loss: 0.2478 - acc: 0.8918 - val_loss: 0.3287 - val_acc: 0.8577\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 746s - loss: 0.2401 - acc: 0.8962 - val_loss: 0.3291 - val_acc: 0.8566\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 745s - loss: 0.2337 - acc: 0.8986 - val_loss: 0.3294 - val_acc: 0.8643\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 719s - loss: 0.2272 - acc: 0.9024 - val_loss: 0.3417 - val_acc: 0.8598\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 740s - loss: 0.2214 - acc: 0.9043 - val_loss: 0.3335 - val_acc: 0.8645\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 747s - loss: 0.2160 - acc: 0.9071 - val_loss: 0.3365 - val_acc: 0.8638\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_9 (InputLayer)             (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 13, 100)       244700      input_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional) (None, 246, 256)      172032      input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional) (None, 13, 256)       234496      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional) (None, 128)           164352      bidirectional_16[0][0]           \n",
      "                                                                   bidirectional_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 128)           512         bidirectional_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 128)           512         bidirectional_15[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_2 (Dot)                      (None, 1)             0           batch_normalization_6[0][0]      \n",
      "                                                                   batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             2           dot_2[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 992s - loss: 0.5190 - acc: 0.7659 - val_loss: 0.4525 - val_acc: 0.8347\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 743s - loss: 0.4362 - acc: 0.8323 - val_loss: 0.6366 - val_acc: 0.8343\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 734s - loss: 0.4122 - acc: 0.8326 - val_loss: 0.5153 - val_acc: 0.8295\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 745s - loss: 0.3912 - acc: 0.8342 - val_loss: 0.4186 - val_acc: 0.8156\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 738s - loss: 0.3703 - acc: 0.8387 - val_loss: 0.3975 - val_acc: 0.8192\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 734s - loss: 0.3502 - acc: 0.8449 - val_loss: 0.3688 - val_acc: 0.8359\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 740s - loss: 0.3326 - acc: 0.8512 - val_loss: 0.3574 - val_acc: 0.8411\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 755s - loss: 0.3171 - acc: 0.8591 - val_loss: 0.3440 - val_acc: 0.8449\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 775s - loss: 0.3033 - acc: 0.8655 - val_loss: 0.3387 - val_acc: 0.8462\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 741s - loss: 0.2935 - acc: 0.8698 - val_loss: 0.3283 - val_acc: 0.8527\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 739s - loss: 0.2823 - acc: 0.8754 - val_loss: 0.3313 - val_acc: 0.8562\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 759s - loss: 0.2732 - acc: 0.8801 - val_loss: 0.3153 - val_acc: 0.8620\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 739s - loss: 0.2658 - acc: 0.8841 - val_loss: 0.3142 - val_acc: 0.8626\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 741s - loss: 0.2566 - acc: 0.8886 - val_loss: 0.3165 - val_acc: 0.8629\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 748s - loss: 0.2499 - acc: 0.8919 - val_loss: 0.3141 - val_acc: 0.8673\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 736s - loss: 0.2436 - acc: 0.8952 - val_loss: 0.3129 - val_acc: 0.8660\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 737s - loss: 0.2367 - acc: 0.8974 - val_loss: 0.3141 - val_acc: 0.8697\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 728s - loss: 0.2311 - acc: 0.9008 - val_loss: 0.3115 - val_acc: 0.8700\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 730s - loss: 0.2233 - acc: 0.9040 - val_loss: 0.3176 - val_acc: 0.8686\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 754s - loss: 0.2183 - acc: 0.9066 - val_loss: 0.3230 - val_acc: 0.8710\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 749s - loss: 0.2121 - acc: 0.9089 - val_loss: 0.3225 - val_acc: 0.8700\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 748s - loss: 0.2081 - acc: 0.9111 - val_loss: 0.3209 - val_acc: 0.8736\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 747s - loss: 0.2030 - acc: 0.9134 - val_loss: 0.3188 - val_acc: 0.8748\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 13, 100)       244700      input_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional) (None, 246, 256)      172032      input_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional) (None, 13, 256)       234496      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional) (None, 128)           164352      bidirectional_19[0][0]           \n",
      "                                                                   bidirectional_20[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 128)           512         bidirectional_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 128)           512         bidirectional_18[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_3 (Dot)                      (None, 1)             0           batch_normalization_8[0][0]      \n",
      "                                                                   batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             2           dot_3[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 1030s - loss: 0.5319 - acc: 0.7627 - val_loss: 0.4843 - val_acc: 0.8265\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 750s - loss: 0.4305 - acc: 0.8322 - val_loss: 0.5408 - val_acc: 0.7803\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 770s - loss: 0.4166 - acc: 0.8332 - val_loss: 0.6904 - val_acc: 0.7478\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 748s - loss: 0.4008 - acc: 0.8343 - val_loss: 0.4721 - val_acc: 0.8111\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 770s - loss: 0.3872 - acc: 0.8361 - val_loss: 0.4956 - val_acc: 0.7834\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 763s - loss: 0.3718 - acc: 0.8396 - val_loss: 0.4487 - val_acc: 0.8195\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 765s - loss: 0.3519 - acc: 0.8451 - val_loss: 0.4154 - val_acc: 0.8232\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 758s - loss: 0.3340 - acc: 0.8510 - val_loss: 0.3829 - val_acc: 0.8335\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 769s - loss: 0.3203 - acc: 0.8566 - val_loss: 0.3882 - val_acc: 0.8347\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 763s - loss: 0.3076 - acc: 0.8625 - val_loss: 0.3583 - val_acc: 0.8405\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 772s - loss: 0.2958 - acc: 0.8693 - val_loss: 0.3639 - val_acc: 0.8413\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 761s - loss: 0.2862 - acc: 0.8729 - val_loss: 0.3437 - val_acc: 0.8449\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 761s - loss: 0.2754 - acc: 0.8788 - val_loss: 0.3516 - val_acc: 0.8440\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 767s - loss: 0.2665 - acc: 0.8839 - val_loss: 0.3347 - val_acc: 0.8532\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 744s - loss: 0.2593 - acc: 0.8871 - val_loss: 0.3393 - val_acc: 0.8555\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 767s - loss: 0.2513 - acc: 0.8902 - val_loss: 0.3298 - val_acc: 0.8580\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 774s - loss: 0.2437 - acc: 0.8943 - val_loss: 0.3277 - val_acc: 0.8620\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 721s - loss: 0.2377 - acc: 0.8965 - val_loss: 0.3273 - val_acc: 0.8603\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 747s - loss: 0.2322 - acc: 0.8995 - val_loss: 0.3276 - val_acc: 0.8659\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 735s - loss: 0.2268 - acc: 0.9025 - val_loss: 0.3294 - val_acc: 0.8626\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 734s - loss: 0.2210 - acc: 0.9049 - val_loss: 0.3349 - val_acc: 0.8668\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 740s - loss: 0.2169 - acc: 0.9069 - val_loss: 0.3325 - val_acc: 0.8657\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 750s - loss: 0.2116 - acc: 0.9092 - val_loss: 0.3290 - val_acc: 0.8634\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 13, 100)       244700      input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional) (None, 246, 256)      172032      input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional) (None, 13, 256)       234496      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional) (None, 128)           164352      bidirectional_22[0][0]           \n",
      "                                                                   bidirectional_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 128)           512         bidirectional_21[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 128)           512         bidirectional_21[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_4 (Dot)                      (None, 1)             0           batch_normalization_10[0][0]     \n",
      "                                                                   batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             2           dot_4[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 900s - loss: 0.5777 - acc: 0.7564 - val_loss: 0.9933 - val_acc: 0.4576\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 650s - loss: 0.4378 - acc: 0.8305 - val_loss: 1.8425 - val_acc: 0.5034\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 644s - loss: 0.4228 - acc: 0.8317 - val_loss: 0.7831 - val_acc: 0.6247\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 647s - loss: 0.4107 - acc: 0.8330 - val_loss: 1.0897 - val_acc: 0.5785\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 634s - loss: 0.3948 - acc: 0.8343 - val_loss: 1.0780 - val_acc: 0.5116\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 644s - loss: 0.3779 - acc: 0.8381 - val_loss: 1.8417 - val_acc: 0.4422\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 654s - loss: 0.3651 - acc: 0.8412 - val_loss: 0.9368 - val_acc: 0.5711\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 722s - loss: 0.3524 - acc: 0.8451 - val_loss: 0.5831 - val_acc: 0.7048\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 702s - loss: 0.3448 - acc: 0.8484 - val_loss: 0.6845 - val_acc: 0.6640\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 668s - loss: 0.3271 - acc: 0.8557 - val_loss: 0.4700 - val_acc: 0.7980\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 698s - loss: 0.3146 - acc: 0.8604 - val_loss: 0.4876 - val_acc: 0.7645\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 725s - loss: 0.3031 - acc: 0.8648 - val_loss: 0.4252 - val_acc: 0.7977\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 656s - loss: 0.2935 - acc: 0.8701 - val_loss: 0.4853 - val_acc: 0.7881\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 662s - loss: 0.2859 - acc: 0.8739 - val_loss: 0.4316 - val_acc: 0.7939\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 667s - loss: 0.2739 - acc: 0.8798 - val_loss: 0.3399 - val_acc: 0.8553\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 667s - loss: 0.2638 - acc: 0.8846 - val_loss: 0.3765 - val_acc: 0.8294\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 664s - loss: 0.2533 - acc: 0.8895 - val_loss: 0.3595 - val_acc: 0.8400\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 623s - loss: 0.2464 - acc: 0.8927 - val_loss: 0.3546 - val_acc: 0.8452\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 749s - loss: 0.2390 - acc: 0.8965 - val_loss: 0.3830 - val_acc: 0.8345\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 726s - loss: 0.2323 - acc: 0.8999 - val_loss: 0.3442 - val_acc: 0.8544\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_15 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 13, 100)       244700      input_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_25 (Bidirectional) (None, 246, 256)      172032      input_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_26 (Bidirectional) (None, 13, 256)       234496      embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional) (None, 128)           164352      bidirectional_25[0][0]           \n",
      "                                                                   bidirectional_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 128)           512         bidirectional_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 128)           512         bidirectional_24[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_5 (Dot)                      (None, 1)             0           batch_normalization_12[0][0]     \n",
      "                                                                   batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             2           dot_5[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 970s - loss: 0.5334 - acc: 0.7643 - val_loss: 0.4652 - val_acc: 0.8307\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 678s - loss: 0.4312 - acc: 0.8317 - val_loss: 0.5467 - val_acc: 0.7598\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 697s - loss: 0.4165 - acc: 0.8325 - val_loss: 1.1110 - val_acc: 0.5537\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 673s - loss: 0.4015 - acc: 0.8339 - val_loss: 0.7519 - val_acc: 0.6484\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 715s - loss: 0.3855 - acc: 0.8366 - val_loss: 0.6231 - val_acc: 0.6751\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 704s - loss: 0.3676 - acc: 0.8400 - val_loss: 0.4838 - val_acc: 0.7854\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_17 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_16 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)          (None, 13, 100)       244700      input_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_28 (Bidirectional) (None, 246, 256)      172032      input_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_29 (Bidirectional) (None, 13, 256)       234496      embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_27 (Bidirectional) (None, 128)           164352      bidirectional_28[0][0]           \n",
      "                                                                   bidirectional_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 128)           512         bidirectional_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 128)           512         bidirectional_27[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_6 (Dot)                      (None, 1)             0           batch_normalization_14[0][0]     \n",
      "                                                                   batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 1)             2           dot_6[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 990s - loss: 0.5798 - acc: 0.7555 - val_loss: 0.6933 - val_acc: 0.6516\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 649s - loss: 0.4379 - acc: 0.8310 - val_loss: 1.3162 - val_acc: 0.5327\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 698s - loss: 0.4228 - acc: 0.8324 - val_loss: 1.3316 - val_acc: 0.5455\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 739s - loss: 0.4095 - acc: 0.8338 - val_loss: 1.5853 - val_acc: 0.4956\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 746s - loss: 0.3976 - acc: 0.8349 - val_loss: 0.8081 - val_acc: 0.6157\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 736s - loss: 0.3828 - acc: 0.8374 - val_loss: 0.7176 - val_acc: 0.6507\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_19 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_18 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)          (None, 13, 100)       244700      input_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_31 (Bidirectional) (None, 246, 256)      172032      input_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_32 (Bidirectional) (None, 13, 256)       234496      embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_30 (Bidirectional) (None, 128)           164352      bidirectional_31[0][0]           \n",
      "                                                                   bidirectional_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 128)           512         bidirectional_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 128)           512         bidirectional_30[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_7 (Dot)                      (None, 1)             0           batch_normalization_16[0][0]     \n",
      "                                                                   batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 1)             2           dot_7[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 992s - loss: 0.5232 - acc: 0.7648 - val_loss: 0.5314 - val_acc: 0.7665\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 775s - loss: 0.4285 - acc: 0.8320 - val_loss: 0.4790 - val_acc: 0.8001\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 763s - loss: 0.4137 - acc: 0.8326 - val_loss: 0.4843 - val_acc: 0.8190\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 775s - loss: 0.3952 - acc: 0.8341 - val_loss: 0.4685 - val_acc: 0.8183\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 782s - loss: 0.3794 - acc: 0.8366 - val_loss: 0.4472 - val_acc: 0.8263\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 778s - loss: 0.3626 - acc: 0.8407 - val_loss: 0.3927 - val_acc: 0.8286\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 776s - loss: 0.3435 - acc: 0.8472 - val_loss: 0.3920 - val_acc: 0.8313\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 740s - loss: 0.3270 - acc: 0.8544 - val_loss: 0.3549 - val_acc: 0.8432\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256705/256705 [==============================] - 713s - loss: 0.3149 - acc: 0.8592 - val_loss: 0.3536 - val_acc: 0.8405\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 657s - loss: 0.3037 - acc: 0.8643 - val_loss: 0.3434 - val_acc: 0.8485\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 653s - loss: 0.2938 - acc: 0.8693 - val_loss: 0.3403 - val_acc: 0.8457\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 657s - loss: 0.2844 - acc: 0.8738 - val_loss: 0.3367 - val_acc: 0.8514\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 656s - loss: 0.2758 - acc: 0.8778 - val_loss: 0.3319 - val_acc: 0.8586\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 659s - loss: 0.2661 - acc: 0.8830 - val_loss: 0.3266 - val_acc: 0.8586\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 656s - loss: 0.2593 - acc: 0.8862 - val_loss: 0.3242 - val_acc: 0.8608\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 665s - loss: 0.2510 - acc: 0.8906 - val_loss: 0.3263 - val_acc: 0.8628\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 678s - loss: 0.2449 - acc: 0.8935 - val_loss: 0.3152 - val_acc: 0.8654\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 720s - loss: 0.2379 - acc: 0.8968 - val_loss: 0.3198 - val_acc: 0.8652\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 726s - loss: 0.2330 - acc: 0.8996 - val_loss: 0.3181 - val_acc: 0.8670\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 700s - loss: 0.2259 - acc: 0.9024 - val_loss: 0.3180 - val_acc: 0.8681\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 647s - loss: 0.2205 - acc: 0.9056 - val_loss: 0.3212 - val_acc: 0.8697\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 655s - loss: 0.2158 - acc: 0.9079 - val_loss: 0.3185 - val_acc: 0.8689\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_21 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_20 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 13, 100)       244700      input_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_34 (Bidirectional) (None, 246, 256)      172032      input_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_35 (Bidirectional) (None, 13, 256)       234496      embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_33 (Bidirectional) (None, 128)           164352      bidirectional_34[0][0]           \n",
      "                                                                   bidirectional_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 128)           512         bidirectional_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 128)           512         bidirectional_33[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_8 (Dot)                      (None, 1)             0           batch_normalization_18[0][0]     \n",
      "                                                                   batch_normalization_19[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             2           dot_8[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 869s - loss: 0.5663 - acc: 0.7599 - val_loss: 0.6532 - val_acc: 0.7361\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 664s - loss: 0.4357 - acc: 0.8312 - val_loss: 1.0954 - val_acc: 0.6903\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 665s - loss: 0.4218 - acc: 0.8328 - val_loss: 0.7922 - val_acc: 0.7426\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 626s - loss: 0.4108 - acc: 0.8336 - val_loss: 0.7094 - val_acc: 0.7349\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 675s - loss: 0.3980 - acc: 0.8355 - val_loss: 0.6921 - val_acc: 0.7808\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 718s - loss: 0.3828 - acc: 0.8379 - val_loss: 0.6013 - val_acc: 0.7998\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 749s - loss: 0.3671 - acc: 0.8403 - val_loss: 0.5041 - val_acc: 0.7803\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 764s - loss: 0.3497 - acc: 0.8462 - val_loss: 0.4827 - val_acc: 0.8172\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 690s - loss: 0.3332 - acc: 0.8524 - val_loss: 0.4851 - val_acc: 0.7893\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 661s - loss: 0.3185 - acc: 0.8586 - val_loss: 0.4224 - val_acc: 0.8187\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 651s - loss: 0.3069 - acc: 0.8639 - val_loss: 0.3875 - val_acc: 0.8321\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 648s - loss: 0.2928 - acc: 0.8701 - val_loss: 0.3869 - val_acc: 0.8361\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 649s - loss: 0.2818 - acc: 0.8761 - val_loss: 0.3795 - val_acc: 0.8258\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 649s - loss: 0.2731 - acc: 0.8801 - val_loss: 0.3715 - val_acc: 0.8378\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 648s - loss: 0.2648 - acc: 0.8835 - val_loss: 0.3695 - val_acc: 0.8447\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 647s - loss: 0.2563 - acc: 0.8886 - val_loss: 0.3620 - val_acc: 0.8457\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 647s - loss: 0.2483 - acc: 0.8921 - val_loss: 0.3631 - val_acc: 0.8500\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 648s - loss: 0.2413 - acc: 0.8959 - val_loss: 0.3504 - val_acc: 0.8546\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 651s - loss: 0.2346 - acc: 0.8985 - val_loss: 0.3564 - val_acc: 0.8492\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 642s - loss: 0.2292 - acc: 0.9015 - val_loss: 0.3586 - val_acc: 0.8514\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 651s - loss: 0.2224 - acc: 0.9045 - val_loss: 0.3610 - val_acc: 0.8560\n",
      "Epoch 22/50\n",
      "256705/256705 [==============================] - 646s - loss: 0.2172 - acc: 0.9064 - val_loss: 0.3472 - val_acc: 0.8594\n",
      "Epoch 23/50\n",
      "256705/256705 [==============================] - 654s - loss: 0.2114 - acc: 0.9092 - val_loss: 0.3548 - val_acc: 0.8629\n",
      "Epoch 24/50\n",
      "256705/256705 [==============================] - 649s - loss: 0.2067 - acc: 0.9120 - val_loss: 0.3520 - val_acc: 0.8585\n",
      "Epoch 25/50\n",
      "256705/256705 [==============================] - 650s - loss: 0.2022 - acc: 0.9136 - val_loss: 0.3589 - val_acc: 0.8628\n",
      "Epoch 26/50\n",
      "256705/256705 [==============================] - 651s - loss: 0.1979 - acc: 0.9158 - val_loss: 0.3602 - val_acc: 0.8613\n",
      "Epoch 27/50\n",
      "256705/256705 [==============================] - 657s - loss: 0.1930 - acc: 0.9179 - val_loss: 0.3744 - val_acc: 0.8594\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_23 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_22 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 13, 100)       244700      input_23[0][0]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "bidirectional_37 (Bidirectional) (None, 246, 256)      172032      input_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_38 (Bidirectional) (None, 13, 256)       234496      embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_36 (Bidirectional) (None, 128)           164352      bidirectional_37[0][0]           \n",
      "                                                                   bidirectional_38[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, 128)           512         bidirectional_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, 128)           512         bidirectional_36[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_9 (Dot)                      (None, 1)             0           batch_normalization_20[0][0]     \n",
      "                                                                   batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             2           dot_9[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 872s - loss: 0.5223 - acc: 0.7830 - val_loss: 0.4581 - val_acc: 0.8338\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 644s - loss: 0.4216 - acc: 0.8321 - val_loss: 0.5691 - val_acc: 0.7230\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 644s - loss: 0.3999 - acc: 0.8331 - val_loss: 0.4431 - val_acc: 0.7987\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 645s - loss: 0.3735 - acc: 0.8367 - val_loss: 0.3801 - val_acc: 0.8232\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 662s - loss: 0.3535 - acc: 0.8435 - val_loss: 0.3698 - val_acc: 0.8298\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 645s - loss: 0.3360 - acc: 0.8496 - val_loss: 0.3629 - val_acc: 0.8355\n",
      "Epoch 7/50\n",
      "256705/256705 [==============================] - 644s - loss: 0.3215 - acc: 0.8560 - val_loss: 0.3536 - val_acc: 0.8382\n",
      "Epoch 8/50\n",
      "256705/256705 [==============================] - 647s - loss: 0.3093 - acc: 0.8624 - val_loss: 0.3374 - val_acc: 0.8454\n",
      "Epoch 9/50\n",
      "256705/256705 [==============================] - 643s - loss: 0.2976 - acc: 0.8679 - val_loss: 0.3348 - val_acc: 0.8480\n",
      "Epoch 10/50\n",
      "256705/256705 [==============================] - 647s - loss: 0.2876 - acc: 0.8731 - val_loss: 0.3280 - val_acc: 0.8534\n",
      "Epoch 11/50\n",
      "256705/256705 [==============================] - 647s - loss: 0.2783 - acc: 0.8768 - val_loss: 0.3262 - val_acc: 0.8563\n",
      "Epoch 12/50\n",
      "256705/256705 [==============================] - 648s - loss: 0.2698 - acc: 0.8813 - val_loss: 0.3410 - val_acc: 0.8521\n",
      "Epoch 13/50\n",
      "256705/256705 [==============================] - 646s - loss: 0.2620 - acc: 0.8852 - val_loss: 0.3244 - val_acc: 0.8618\n",
      "Epoch 14/50\n",
      "256705/256705 [==============================] - 649s - loss: 0.2537 - acc: 0.8899 - val_loss: 0.3203 - val_acc: 0.8614\n",
      "Epoch 15/50\n",
      "256705/256705 [==============================] - 639s - loss: 0.2473 - acc: 0.8928 - val_loss: 0.3236 - val_acc: 0.8614\n",
      "Epoch 16/50\n",
      "256705/256705 [==============================] - 645s - loss: 0.2404 - acc: 0.8956 - val_loss: 0.3185 - val_acc: 0.8631\n",
      "Epoch 17/50\n",
      "256705/256705 [==============================] - 648s - loss: 0.2351 - acc: 0.8984 - val_loss: 0.3289 - val_acc: 0.8594\n",
      "Epoch 18/50\n",
      "256705/256705 [==============================] - 644s - loss: 0.2286 - acc: 0.9010 - val_loss: 0.3213 - val_acc: 0.8630\n",
      "Epoch 19/50\n",
      "256705/256705 [==============================] - 642s - loss: 0.2240 - acc: 0.9035 - val_loss: 0.3227 - val_acc: 0.8668\n",
      "Epoch 20/50\n",
      "256705/256705 [==============================] - 655s - loss: 0.2188 - acc: 0.9060 - val_loss: 0.3211 - val_acc: 0.8667\n",
      "Epoch 21/50\n",
      "256705/256705 [==============================] - 734s - loss: 0.2126 - acc: 0.9091 - val_loss: 0.3259 - val_acc: 0.8656\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_25 (InputLayer)            (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_24 (InputLayer)            (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)         (None, 13, 100)       244700      input_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_40 (Bidirectional) (None, 246, 256)      172032      input_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_41 (Bidirectional) (None, 13, 256)       234496      embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_39 (Bidirectional) (None, 128)           164352      bidirectional_40[0][0]           \n",
      "                                                                   bidirectional_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNor (None, 128)           512         bidirectional_39[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNor (None, 128)           512         bidirectional_39[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dot_10 (Dot)                     (None, 1)             0           batch_normalization_22[0][0]     \n",
      "                                                                   batch_normalization_23[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             2           dot_10[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 816,606\n",
      "Trainable params: 816,094\n",
      "Non-trainable params: 512\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 256705 samples, validate on 13511 samples\n",
      "Epoch 1/50\n",
      "256705/256705 [==============================] - 935s - loss: 0.5581 - acc: 0.7611 - val_loss: 0.5905 - val_acc: 0.7149\n",
      "Epoch 2/50\n",
      "256705/256705 [==============================] - 708s - loss: 0.4367 - acc: 0.8313 - val_loss: 1.0057 - val_acc: 0.5179\n",
      "Epoch 3/50\n",
      "256705/256705 [==============================] - 721s - loss: 0.4213 - acc: 0.8328 - val_loss: 0.7931 - val_acc: 0.6198\n",
      "Epoch 4/50\n",
      "256705/256705 [==============================] - 700s - loss: 0.4087 - acc: 0.8337 - val_loss: 0.7926 - val_acc: 0.6192\n",
      "Epoch 5/50\n",
      "256705/256705 [==============================] - 720s - loss: 0.3948 - acc: 0.8354 - val_loss: 0.6399 - val_acc: 0.6858\n",
      "Epoch 6/50\n",
      "256705/256705 [==============================] - 713s - loss: 0.3799 - acc: 0.8373 - val_loss: 0.7551 - val_acc: 0.6409\n"
     ]
    }
   ],
   "source": [
    "# model with shared weight \n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "emb_size = 100\n",
    "batch_size = 512\n",
    "epochs = 50\n",
    "for i in range(10):\n",
    "    share_lstm1 = Bidirectional(LSTM(64,dropout=0.2))\n",
    "    mfcc_input = Input(shape=(train_mfcc.shape[1],train_mfcc.shape[2]))\n",
    "    mfcc_lstm1 = Bidirectional(LSTM(128,dropout=0.2,return_sequences=True))(mfcc_input)\n",
    "    mfcc_lstm2 = share_lstm1(mfcc_lstm1)\n",
    "    mfcc_bn2 = BatchNormalization()(mfcc_lstm2)\n",
    "    \n",
    "    caption_input = Input(shape=(13,))\n",
    "    emb = Embedding(len(tokenizer.word_index)+1 ,output_dim= emb_size, \n",
    "                    input_length=max_length,trainable=True)(caption_input)\n",
    "    caption_lstm1 = Bidirectional(LSTM(128,dropout=0.2,return_sequences = True))(emb)\n",
    "    caption_lstm2 = share_lstm1(caption_lstm1)\n",
    "    caption_bn2 = BatchNormalization()(caption_lstm2)\n",
    "    \n",
    "    merge = keras.layers.dot([mfcc_bn2, caption_bn2],1)\n",
    "    output_dense = Dense(1,activation=\"sigmoid\")(merge)\n",
    "    model = Model(inputs=[mfcc_input, caption_input], outputs=output_dense)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    # training\n",
    "    total_sample_size = len(ground_truth)\n",
    "    random_index = np.random.choice(total_sample_size,total_sample_size, replace=False)\n",
    "    \n",
    "    input_mfcc = train_mfcc[random_index]\n",
    "    input_caption = train_caption[random_index]\n",
    "    input_ground_truth = np.array(ground_truth)[random_index]\n",
    "\n",
    "    hist = History()\n",
    "#     check_save  = ModelCheckpoint(\"models/model_1v3-{epoch:05d}-{val_acc:.5f}.h5\",monitor='val_acc',save_best_only=True)\n",
    "    check_save  = ModelCheckpoint(\"models/model_1V5_\"+str(batch_size)+\"_\"+str(i)+\"_share1_bn.h5\",monitor='val_acc',save_best_only=True)\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=4)\n",
    "    model.fit([input_mfcc, input_caption], input_ground_truth,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.05,\n",
    "             callbacks=[check_save, hist, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
