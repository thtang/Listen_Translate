{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "import gensim\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, GRU, Embedding, Bidirectional, BatchNormalization, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import History ,ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.merge import add, dot, concatenate\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45036 個 training 音檔\n",
      "2000 個 testing 音檔\n",
      "max langth of wav: 246\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(\"data/train.data\")\n",
    "test_data = np.load(\"data/test.data\")\n",
    "print(len(train_data),\"個 training 音檔\")\n",
    "print(len(test_data),\"個 testing 音檔\")\n",
    "max_frame_length = np.max([len(sample) for sample in train_data])\n",
    "print(\"max langth of wav:\",max_frame_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# downsampling for mfcc augmentation\n",
    "aug_train_data = []\n",
    "for data in train_data:\n",
    "    aug_train_data.append(data[::2])\n",
    "train_data = train_data + aug_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90072"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load caption\n",
    "with open(\"data/train.caption\",\"r\") as f:\n",
    "    train_caption = f.readlines()\n",
    "    train_caption = [sent.strip() for sent in train_caption]\n",
    "    train_sentences = [sent.split(\" \") for sent in train_caption]\n",
    "with open(\"data/test.csv\",\"r\") as f:\n",
    "    test_choice = f.readlines()\n",
    "    test_choice = [sent.strip() for sent in test_choice]\n",
    "    test_corpus = \",\".join(test_choice)\n",
    "    test_sentences = [sent.split(\" \") for sent in test_corpus.split(\",\")]\n",
    "    test_corpus = test_corpus.replace(\",\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of token in caption: 2446\n"
     ]
    }
   ],
   "source": [
    "# chinese character level tokenizer\n",
    "tokenizer = Tokenizer(num_words=None,filters='\\n', lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(train_caption + [test_corpus])\n",
    "print(\"number of token in caption:\", len(tokenizer.word_index))\n",
    "inv_map = {v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 13\n"
     ]
    }
   ],
   "source": [
    "train_caption_sequences = tokenizer.texts_to_sequences(train_caption)\n",
    "max_length = np.max([len(i) for i in train_caption_sequences])\n",
    "print(\"max length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['對', '了', '文', '隆', '哥']\n",
      "(90072, 13)\n",
      "(90072, 246, 39)\n"
     ]
    }
   ],
   "source": [
    "# pad sequence\n",
    "train_caption_pad = pad_sequences(train_caption_sequences + train_caption_sequences, maxlen=max_length)\n",
    "train_data_pad = pad_sequences(train_data, maxlen=max_frame_length,dtype='float32')\n",
    "# revert\n",
    "print([inv_map[i] for i in  train_caption_pad[1] if i != 0])\n",
    "print(train_caption_pad.shape)\n",
    "print(train_data_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 13)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 246, 39)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 13, 100)       244700      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 246, 256)      129024      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional)  (None, 13, 256)       175872      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 128)           123264      bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional)  (None, 128)           123264      bidirectional_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1)             0           bidirectional_2[0][0]            \n",
      "                                                                   bidirectional_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             2           dot_1[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 796,126\n",
      "Trainable params: 796,126\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 513408 samples, validate on 4504 samples\n",
      "Epoch 1/1\n",
      "513408/513408 [==============================] - 1155s - loss: 0.4291 - acc: 0.8324 - val_loss: 1.3482 - val_acc: 0.0353\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "emb_size = 100\n",
    "batch_size = 512\n",
    "epochs = 8\n",
    "# ======= train valid split =======\n",
    "train_caption_pad, valid_caption_pad, train_data_pad, valid_data_pad = train_test_split(train_caption_pad, train_data_pad, test_size=0.05)\n",
    "\n",
    "\n",
    "# ======= define model ======\n",
    "mfcc_input = Input(shape=(246,39))\n",
    "mfcc_lstm1 = Bidirectional(GRU(128,dropout=0.2, return_sequences=True))(mfcc_input)\n",
    "mfcc_lstm2 = Bidirectional(GRU(64,dropout=0.2))(mfcc_lstm1)\n",
    "\n",
    "caption_input = Input(shape=(13,))\n",
    "emb = Embedding(len(tokenizer.word_index)+1 ,output_dim= emb_size, \n",
    "                input_length=max_length,trainable=True)(caption_input)\n",
    "caption_lstm1 = Bidirectional(GRU(128,dropout=0.2, return_sequences = True))(emb)\n",
    "caption_lstm2 = Bidirectional(GRU(64,dropout=0.2))(caption_lstm1)\n",
    "\n",
    "merge = keras.layers.dot([mfcc_lstm2, caption_lstm2],1)\n",
    "output_dense = Dense(1,activation=\"sigmoid\")(merge)\n",
    "model = Model(inputs=[mfcc_input, caption_input], outputs=output_dense)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "for i in range(0,20):\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        # build training tensor (truth and fake for binary calssification)\n",
    "        false_caption = []\n",
    "        false_mfcc = train_data_pad\n",
    "        true_caption = train_caption_pad\n",
    "        true_mfcc = train_data_pad\n",
    "\n",
    "        ## random rolling way for negative sampling \n",
    "        roll_sample = np.random.choice(len(train_caption_pad),5, replace=False)\n",
    "        false_caption = np.concatenate((np.roll(train_caption_pad,roll_sample[0],axis=0),\n",
    "                                        np.roll(train_caption_pad,roll_sample[1],axis=0),\n",
    "                                       np.roll(train_caption_pad,roll_sample[2],axis=0),\n",
    "                                       np.roll(train_caption_pad,roll_sample[3],axis=0),\n",
    "                                       np.roll(train_caption_pad,roll_sample[4],axis=0)))\n",
    "        false_mfcc = np.concatenate((train_data_pad,\n",
    "                                     train_data_pad,\n",
    "                                     train_data_pad,\n",
    "                                     train_data_pad,\n",
    "                                     train_data_pad))\n",
    "        true_caption = train_caption_pad\n",
    "        true_mfcc = train_data_pad\n",
    "\n",
    "        ground_truth = [ 1 for _ in range(len(true_caption))] + [0 for _ in range(len(false_caption))]\n",
    "        train_mfcc = np.concatenate((true_mfcc, np.array(false_mfcc)))\n",
    "        train_caption = np.concatenate((true_caption, np.array(false_caption)))\n",
    "\n",
    "        total_sample_size = len(ground_truth)\n",
    "        random_index = np.random.choice(total_sample_size,total_sample_size, replace=False)\n",
    "\n",
    "        input_mfcc = train_mfcc[random_index]\n",
    "        input_caption = train_caption[random_index]\n",
    "        input_ground_truth = np.array(ground_truth)[random_index]\n",
    "\n",
    "        hist = History()\n",
    "    #     check_save  = ModelCheckpoint(\"models/model_1v3-{epoch:05d}-{val_acc:.5f}.h5\",monitor='val_acc',save_best_only=True)\n",
    "        check_save  = ModelCheckpoint(\"models/model5_randomAug1_1V5_2layers_\"+str(batch_size)+\"_\"+str(i)+str(epoch)+\".h5\")\n",
    "        model.fit([input_mfcc, input_caption], input_ground_truth,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data = ([valid_data_pad, valid_caption_pad],np.ones(len(valid_data_pad))),\n",
    "                  epochs=1, callbacks=[check_save, hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(len(valid_data_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513408, 246, 39)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 13)\n",
      "(8000, 246, 39)\n"
     ]
    }
   ],
   "source": [
    "test_caption_sequences =  tokenizer.texts_to_sequences([\" \".join(sample) for sample in test_sentences])\n",
    "\n",
    "# pad sequence\n",
    "test_caption_pad = pad_sequences(test_caption_sequences, maxlen=max_length)\n",
    "test_data_pad = pad_sequences(test_data, maxlen=max_frame_length,dtype='float32')\n",
    "test_data_pad_expand = np.repeat(test_data_pad, 4,axis=0)\n",
    "# revert\n",
    "print(test_caption_pad.shape)\n",
    "print(test_data_pad_expand .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "p = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    model = load_model(\"models/model_1V5_512_\"+str(i)+\"_share1_bn.h5\")\n",
    "    prediction = model.predict([test_data_pad_expand,test_caption_pad])\n",
    "    p.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load submit\n",
    "pred_y_prob = np.sum(p,axis = 0)\n",
    "sample_submit = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "pred_y = np.argmax(pred_y_prob.reshape(-1,4),axis=1)\n",
    "sample_submit[\"answer\"] = pred_y\n",
    "sample_submit.to_csv(\"final_submission.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88859, 63793, 49169,  9478, 49504])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
